{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import re\n",
    "from pandas.errors import EmptyDataError\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "plt.rcParams.update({'font.size': 20})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"orin_agx\"\n",
    "# device = \"orin_nano\"\n",
    "device = \"**\"\n",
    "tdp = \"*w\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "model_dirs = []\n",
    "for app in [\"MLCChat\", \"LLMFarmEval\", \"LlamaCpp\"]:\n",
    "    if app == \"MLCChat\":\n",
    "        model = \"**/\"\n",
    "    else:\n",
    "        model = \"**/**\"\n",
    "    model_dirs.extend(glob.glob(f\"../experiment_outputs/{device}/{app}/{model}/{tdp}/\"))\n",
    "\n",
    "app = \"**\"\n",
    "model_dirs = sorted(model_dirs)\n",
    "print(f\"{len(model_dirs)} found!\")\n",
    "for model_dir in model_dirs[:]:\n",
    "    print(model_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dirs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_stats_plots(dir):\n",
    "    for d in dir:\n",
    "        # print(d)\n",
    "        if not os.path.exists(os.path.join(d, \"results_model_inference_measurements.csv\")):\n",
    "            print(d)\n",
    "            !python ../src/report_performance.py -p {d}\n",
    "generate_stats_plots(model_dirs[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "files.extend(glob.glob(f\"../experiment_outputs/{device}/{app}/**/{tdp}/results_model_inference_measurements.csv\"))\n",
    "files.extend(glob.glob(f\"../experiment_outputs/{device}/{app}/**/**/{tdp}/results_model_inference_measurements.csv\"))\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for f in files:\n",
    "    print(f)\n",
    "    le_device = re.search(\"experiment_outputs/(.*?)/(.*?)/.*/(.*?w)/\", f).group(1)\n",
    "    application = re.search(\"experiment_outputs/(.*?)/(.*?)/.*/(.*?w)/\", f).group(2)\n",
    "    tdp = re.search(\"experiment_outputs/(.*?)/(.*?)/.*/(.*?w)/\", f).group(3)\n",
    "    model_regex = f\"{application}/(.*)/\\d+w/\"\n",
    "    context_size = 2048\n",
    "    max_gen_len = 512\n",
    "    try:\n",
    "        df = pd.read_csv(f)\n",
    "        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        print(f\"dropping {df.isna().sum().sum()} NaNs\")\n",
    "        df.dropna(inplace=True)\n",
    "        df[\"context_size\"] = context_size\n",
    "        df[\"max_gen_len\"] = max_gen_len\n",
    "        df[\"batch_size\"] = 1024\n",
    "        full_model_name = re.search(model_regex, f).group(1)\n",
    "        model_name = full_model_name.split('/')[0].split('-q')[0]\n",
    "        df[\"model\"] = model_name\n",
    "        df[\"quantisation\"] = full_model_name.split(\"-\")[-1].split('.')[0]\n",
    "        df[\"device\"] = le_device\n",
    "        df[\"app\"] = application\n",
    "        df['tdp'] = tdp\n",
    "        dfs.append(df)\n",
    "    except EmptyDataError as e:\n",
    "        print(f\"Empty file: {f}\")\n",
    "        continue\n",
    "\n",
    "df = pd.concat(dfs)\n",
    "grouped_df = df.groupby([\"device\", \"tdp\", \"app\", \"model\", \"quantisation\"])\n",
    "grouped_df_tps = grouped_df.mean()\n",
    "grouped_df_tps[\"tps_std\"] = grouped_df[['tps']].std()\n",
    "grouped_df_tps = grouped_df_tps[[\"tps\", \"tps_std\"]].reset_index()\n",
    "display(grouped_df_tps[:5])\n",
    "\n",
    "# These are only accessible when you have energy measurements from orin agx and nano, otherwise pick your columns.\n",
    "energy_cols = [\"energy_pt VDD_GPU_SOC (mWh)\",\n",
    "               \"energy_pt VDD_CPU_CV (mWh)\",\n",
    "               \"energy_pt VIN_SYS_5V0 (mWh)\",\n",
    "               \"energy_pt NC (mWh)\",\n",
    "               \"energy_pt VDDQ_VDD2_1V8AO (mWh)\",\n",
    "               \"energy_pt VDD_IN (mWh)\",\n",
    "               \"energy_pt VDD_CPU_GPU_CV (mWh)\",\n",
    "               \"energy_pt VDD_SOC (mWh)\"]\n",
    "discharge_cols = [col.replace(\"energy\", \"discharge\") for col in energy_cols]\n",
    "all_energy_cols = energy_cols #+ discharge_cols\n",
    "relevant_energy_cols = [\"input_tokens\", \"output_tokens\", \"tps\"] + all_energy_cols\n",
    "\n",
    "df_energy = df\n",
    "for col in all_energy_cols:\n",
    "    if col in df_energy.columns:\n",
    "        new_col = col.replace(\"pt\", \"total\")\n",
    "        total_energy = df_energy[col] * df_energy[\"output_tokens\"]\n",
    "        df_energy[new_col] = total_energy\n",
    "display(df_energy[:5])\n",
    "\n",
    "# Pick the columns that you have populated here.\n",
    "# df_energy[\"total_energy\"] = df_energy[[col for col in df_energy.columns if col.startswith(\"energy_total\")]].sum(axis=1)\n",
    "df_energy[\"total_energy\"] = df_energy[[\"energy_pt VDD_GPU_SOC (mWh)\", \"energy_pt VDD_CPU_CV (mWh)\",\n",
    "                                       \"energy_pt VDD_CPU_GPU_CV (mWh)\", \"energy_pt VDD_SOC (mWh)\"]].sum(axis=1)\n",
    "# df_energy[\"total_energy\"] = df_energy[[\"energy_pt VIN_SYS_5V0 (mWh)\", \"energy_pt VDD_IN (mWh)\"]].sum(axis=1)\n",
    "\n",
    "df_energy_grouped = df_energy.groupby([\"device\", \"tdp\", \"app\", \"model\", \"quantisation\", \"iteration\"])[[\"total_energy\", \"output_tokens\"]]\n",
    "grouped_df_energy = df_energy_grouped.sum()\n",
    "grouped_df_energy[\"ept\"] = grouped_df_energy[\"total_energy\"] / grouped_df_energy[\"output_tokens\"]\n",
    "ggde = grouped_df_energy.reset_index().groupby([\"device\", \"tdp\", \"app\", \"model\", \"quantisation\"])\n",
    "grouped_df_energy_total = ggde[[\"total_energy\", \"ept\", \"output_tokens\"]].mean()\n",
    "grouped_df_energy_total = pd.concat([\n",
    "        grouped_df_energy_total,\n",
    "        ggde[[\"total_energy\", \"ept\", \"output_tokens\"]].std().rename(columns={\"total_energy\": \"total_energy_std\", \"ept\": \"ept_std\", \"output_tokens\": \"output_tokens_std\"})\n",
    "    ], axis=1)\n",
    "\n",
    "display(grouped_df_energy_total[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlc-chat-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
